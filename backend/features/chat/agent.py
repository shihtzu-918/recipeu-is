# backend/features/chat/agent.py
"""
Chat Agent - Adaptive RAG
"""
import os
import time
from typing import TypedDict, List, Literal
from langgraph.graph import StateGraph, END
from langchain_core.documents import Document

# prompts.pyì—ì„œ í”„ë¡¬í”„íŠ¸ import
from .prompts import REWRITE_PROMPT, GRADE_PROMPT, GENERATE_PROMPT
from services.search import get_search_service


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í† í° ì‚¬ìš©ëŸ‰ ì¶”ì  í—¬í¼ í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìš”ì²­ë³„ í† í° ëˆ„ì  (ìš”ì²­ë‹¹ ì´ˆê¸°í™”ë¨)
_token_accumulator: dict = {"prompt": 0, "completion": 0, "total": 0}
# ë…¸ë“œë³„ í† í° ì •ë³´ ì €ì¥ (ë…¸ë“œëª… -> {prompt, completion, total})
_node_tokens: dict = {}

def print_token_usage(response, context_name: str = "LLM"):
    """LLM ì‘ë‹µì—ì„œ ì‹¤ì œ í† í° ì‚¬ìš©ëŸ‰ ì¶œë ¥ ë° ëˆ„ì  (ê°œì„  ë²„ì „)"""
    print(f"\n{'='*60}")
    print(f"[{context_name}] HCX API í† í° ì‚¬ìš©ëŸ‰ (ì‹¤ì¸¡)")
    print(f"{'='*60}")

    # ê°œì„ : usage_metadata ìš°ì„  í™•ì¸ (LangChain í‘œì¤€)
    usage = None
    source = ""

    if hasattr(response, 'usage_metadata'):
        usage = response.usage_metadata
        source = "usage_metadata"
    elif hasattr(response, 'response_metadata'):
        usage = response.response_metadata.get('token_usage')
        source = "response_metadata.token_usage"

    if usage:
        # ê°œì„ : ì†ŒìŠ¤ì— ë”°ë¼ í•„ë“œëª… ë¶„ê¸°
        if source == "usage_metadata":
            prompt_tokens = usage.get('input_tokens', 0)
            completion_tokens = usage.get('output_tokens', 0)
            total_tokens = usage.get('total_tokens', 0)
        else:
            prompt_tokens = usage.get('prompt_tokens', 0)
            completion_tokens = usage.get('completion_tokens', 0)
            total_tokens = usage.get('total_tokens', 0)

        # Fallback: total_tokensì´ ì—†ìœ¼ë©´ ê³„ì‚°
        if total_tokens == 0:
            total_tokens = prompt_tokens + completion_tokens

        # ì „ì²´ ëˆ„ì 
        _token_accumulator["prompt"] += prompt_tokens
        _token_accumulator["completion"] += completion_tokens
        _token_accumulator["total"] += total_tokens

        # ë…¸ë“œë³„ ì €ì¥ (ëˆ„ì )
        if context_name not in _node_tokens:
            _node_tokens[context_name] = {"prompt": 0, "completion": 0, "total": 0}
        _node_tokens[context_name]["prompt"] += prompt_tokens
        _node_tokens[context_name]["completion"] += completion_tokens
        _node_tokens[context_name]["total"] += total_tokens

        print(f"ğŸ“¥ ì…ë ¥ í† í° (prompt):     {prompt_tokens:,} tokens")
        print(f"ğŸ“¤ ì¶œë ¥ í† í° (completion): {completion_tokens:,} tokens")
        print(f"ğŸ“Š ì´ í† í° (total):        {total_tokens:,} tokens")
        print(f"ğŸ” í† í° ì†ŒìŠ¤: {source}")
    else:
        # âš ï¸ usageê°€ ì—†ì„ ë•Œë„ ì¼ë‹¨ 0ìœ¼ë¡œ ì§‘ê³„ì— í¬í•¨ (ëˆ„ë½ ë°©ì§€)
        print(f"âš ï¸  í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì§‘ê³„ ëˆ„ë½ ê°€ëŠ¥)")
        print(f"ğŸ” í† í° ì†ŒìŠ¤: âŒ NOT FOUND")

        # ë…¸ë“œëŠ” ë“±ë¡í•˜ë˜ í† í°ì€ 0ìœ¼ë¡œ
        if context_name not in _node_tokens:
            _node_tokens[context_name] = {"prompt": 0, "completion": 0, "total": 0}

        print(f"ğŸ“¥ ì…ë ¥ í† í° (prompt):     âŒ ì¸¡ì • ë¶ˆê°€")
        print(f"ğŸ“¤ ì¶œë ¥ í† í° (completion): âŒ ì¸¡ì • ë¶ˆê°€")
        print(f"ğŸ“Š ì´ í† í° (total):        âŒ ì¸¡ì • ë¶ˆê°€")

        # ë””ë²„ê¹…ìš©
        print(f"ì‘ë‹µ ê°ì²´ íƒ€ì…: {type(response)}")
        if hasattr(response, 'response_metadata'):
            print(f"response_metadata: {response.response_metadata}")
        if hasattr(response, 'usage_metadata'):
            print(f"usage_metadata: {response.usage_metadata}")

    print(f"{'='*60}\n")

def print_token_summary():
    """ëˆ„ì ëœ í† í° ì‚¬ìš©ëŸ‰ ìš”ì•½ ì¶œë ¥ (ë§ˆí¬ë‹¤ìš´ í˜•ì‹)"""
    if _token_accumulator["total"] == 0:
        return

    print(f"\n{'ğŸ”·'*30}")
    print(f"{'  '*10}ğŸ“Š ì „ì²´ í† í° ì‚¬ìš©ëŸ‰ ìš”ì•½")
    print(f"{'ğŸ”·'*30}")
    print(f"ğŸ“¥ ì´ ì…ë ¥ í† í° (prompt):     {_token_accumulator['prompt']:,} tokens")
    print(f"ğŸ“¤ ì´ ì¶œë ¥ í† í° (completion): {_token_accumulator['completion']:,} tokens")
    print(f"ğŸ“Š ì´í•© (total):              {_token_accumulator['total']:,} tokens")
    print(f"{'ğŸ”·'*30}\n")

    # 1) ë…¸ë“œë³„ í† í°/ì‹œê°„ ìš”ì•½ í‘œ (ë§ˆí¬ë‹¤ìš´)
    print("\n" + "="*100)
    print("- ğŸ“‹ ë…¸ë“œë³„ ìƒì„¸ ìš”ì•½\n")
    print("| Step | Node | ì„¤ëª… | Prompt Tokens | Completion Tokens | Total Tokens | Latency(s) |")
    print("|------|------|------|---------------|-------------------|--------------|------------|")

    # ë…¸ë“œ ìˆœì„œ ë° ë©”íƒ€ë°ì´í„° ì •ì˜ (timing_key ì¤‘ë³µ ë…¸ë“œ ì œê±°)
    node_order = ["ì±„íŒ… ì˜ë„ ê°ì§€", "ê´€ë ¨ì„± ì²´í¬", "ì¿¼ë¦¬ ì¬ì‘ì„±", "retrieve", "check_constraints", "ê´€ë ¨ì„± í‰ê°€", "web_search", "ë‹µë³€ ìƒì„±"]
    node_metadata = {
        "ì±„íŒ… ì˜ë„ ê°ì§€": {"step": "-", "desc": "ì±„íŒ… ì˜ë„ ê°ì§€", "timing_key": "ì±„íŒ… ì˜ë„ ê°ì§€"},
        "ê´€ë ¨ì„± ì²´í¬": {"step": "0", "desc": "ë ˆì‹œí”¼ ê´€ë ¨ì„± ì²´í¬", "timing_key": "check_relevance"},
        "ì¿¼ë¦¬ ì¬ì‘ì„±": {"step": "1", "desc": "ì¿¼ë¦¬ ì¬ì‘ì„±", "timing_key": "rewrite"},
        "retrieve": {"step": "2", "desc": "RAG ê²€ìƒ‰", "timing_key": "retrieve"},
        "check_constraints": {"step": "2.5", "desc": "ì œì•½ ì¡°ê±´ ì²´í¬", "timing_key": "check_constraints"},
        "ê´€ë ¨ì„± í‰ê°€": {"step": "3", "desc": "ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€", "timing_key": "grade"},
        "web_search": {"step": "4", "desc": "ì›¹ ê²€ìƒ‰", "timing_key": "web_search"},
        "ë‹µë³€ ìƒì„±": {"step": "5", "desc": "ë‹µë³€ ìƒì„±", "timing_key": "generate"},
    }

    # timing_key ì¤‘ë³µ ë°©ì§€ ë° ë…¸ë“œ ìˆœì„œëŒ€ë¡œ ì¶œë ¥
    printed_nodes = set()
    printed_timing_keys = set()

    for node_name in node_order:
        tokens = _node_tokens.get(node_name, {"prompt": 0, "completion": 0, "total": 0})
        meta = node_metadata.get(node_name, {"step": "-", "desc": node_name, "timing_key": node_name})
        timing_key = meta["timing_key"]

        # ì´ë¯¸ ì¶œë ¥ëœ timing_keyì´ê³  í† í°ì´ ì—†ìœ¼ë©´ ìŠ¤í‚µ (ì¤‘ë³µ ë°©ì§€)
        if timing_key in printed_timing_keys and tokens["total"] == 0:
            continue

        timing_ms = _node_timings.get(timing_key, 0)
        timing_sec = timing_ms / 1000 if timing_ms else 0

        if tokens["total"] > 0 or timing_sec > 0:
            prompt_str = str(tokens["prompt"]) if tokens["prompt"] > 0 else "-"
            completion_str = str(tokens["completion"]) if tokens["completion"] > 0 else "-"
            total_str = str(tokens["total"]) if tokens["total"] > 0 else "-"
            latency_str = f"{timing_sec:.1f}" if timing_sec > 0 else "-"
            print(f"| {meta['step']} | {node_name} | {meta['desc']} | {prompt_str} | {completion_str} | {total_str} | {latency_str} |")
            printed_nodes.add(node_name)
            printed_timing_keys.add(timing_key)

    # 2) node_orderì— ì—†ì§€ë§Œ ê¸°ë¡ëœ ë…¸ë“œë“¤ ì¶œë ¥ (ë©”íƒ€ë°ì´í„° í™œìš©)
    for node_name, tokens in _node_tokens.items():
        if node_name not in printed_nodes:
            meta = node_metadata.get(node_name, {"step": "-", "desc": node_name, "timing_key": node_name})
            timing_ms = _node_timings.get(node_name, 0)
            timing_sec = timing_ms / 1000 if timing_ms else 0

            if tokens["total"] > 0 or timing_sec > 0:
                prompt_str = str(tokens["prompt"]) if tokens["prompt"] > 0 else "-"
                completion_str = str(tokens["completion"]) if tokens["completion"] > 0 else "-"
                total_str = str(tokens["total"]) if tokens["total"] > 0 else "-"
                latency_str = f"{timing_sec:.1f}" if timing_sec > 0 else "-"
                print(f"| {meta['step']} | {node_name} | {meta['desc']} | {prompt_str} | {completion_str} | {total_str} | {latency_str} |")
                printed_nodes.add(node_name)

    # 2) ì „ì²´ í•©ê³„ ìš”ì•½ í‘œ (ë§ˆí¬ë‹¤ìš´)
    print("\n- ğŸ“Š ì „ì²´ í•©ê³„ ìš”ì•½\n")
    print("| êµ¬ë¶„ | Prompt Tokens | Completion Tokens | Total Tokens |")
    print("|------|---------------|-------------------|--------------|")
    print(f"| í•©ê³„ | {_token_accumulator['prompt']:,} | {_token_accumulator['completion']:,} | {_token_accumulator['total']:,} |")

    # 3) ì„±ëŠ¥ ë³‘ëª© í‘œ: ë™ì‘ í”Œë¡œìš° ìˆœì„œëŒ€ë¡œ (ë§ˆí¬ë‹¤ìš´)
    if _node_timings:
        print("\n- âš¡ ì„±ëŠ¥ ë³‘ëª© ë¶„ì„\n")
        print("| ë™ì‘ | Node | Latency(s) | ë¹„ìœ¨ |")
        print("|------|------|------------|------|")

        # ë™ì‘ í”Œë¡œìš° ìˆœì„œ ì •ì˜
        node_order = ["ì±„íŒ… ì˜ë„ ê°ì§€", "check_relevance", "rewrite", "retrieve", "check_constraints", "grade", "web_search", "generate"]
        total_time = sum(_node_timings.values())

        printed_timing_nodes = set()
        order_counter = 1

        # node_orderì— ìˆëŠ” ë…¸ë“œ ë¨¼ì € ì¶œë ¥
        for node_name in node_order:
            ms = _node_timings.get(node_name, 0)
            if ms > 0:
                sec = ms / 1000
                ratio = (ms / total_time * 100) if total_time > 0 else 0
                print(f"| {order_counter} | {node_name} | {sec:.1f} | ~{ratio:.0f}% |")
                printed_timing_nodes.add(node_name)
                order_counter += 1

        # node_orderì— ì—†ëŠ” ë…¸ë“œë“¤ë„ ì¶œë ¥ (ì±„íŒ… ì˜ë„ ê°ì§€, ë ˆì‹œí”¼ ìˆ˜ì • ë“±)
        for node_name, ms in _node_timings.items():
            if node_name not in printed_timing_nodes and ms > 0:
                sec = ms / 1000
                ratio = (ms / total_time * 100) if total_time > 0 else 0
                print(f"| {order_counter} | {node_name} | {sec:.1f} | ~{ratio:.0f}% |")
                order_counter += 1

        # ì´ ì†Œìš” ì‹œê°„ ì¶”ê°€
        total_sec = total_time / 1000
        print(f"| - | **TOTAL** | **{total_sec:.1f}** | **100%** |")

        print("="*100 + "\n")

    # ì´ˆê¸°í™”
    _token_accumulator["prompt"] = 0
    _token_accumulator["completion"] = 0
    _token_accumulator["total"] = 0
    _node_tokens.clear()
    _node_timings.clear()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë…¸ë“œë³„ íƒ€ì´ë° ë˜í¼
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ëˆ„ì  íƒ€ì´ë°ì„ ì €ì¥í•  ì „ì—­ ë”•ì…”ë„ˆë¦¬ (ìš”ì²­ë‹¹ ì´ˆê¸°í™”ë¨)
_node_timings: dict = {}

def timed_node(name: str, fn):
    """ë…¸ë“œ í•¨ìˆ˜ë¥¼ ê°ì‹¸ì„œ ì‹¤í–‰ ì‹œê°„ì„ ìë™ ë¡œê¹…"""
    def wrapper(state: "ChatAgentState") -> "ChatAgentState":
        start = time.time()
        result = fn(state)
        elapsed_ms = (time.time() - start) * 1000
        _node_timings[name] = elapsed_ms
        elapsed_sec = elapsed_ms / 1000
        print(f"  â±ï¸  [Node: {name}] {elapsed_sec:.1f}ì´ˆ")
        return result
    return wrapper


class ChatAgentState(TypedDict):
    """Agent ìƒíƒœ"""
    question: str
    original_question: str
    chat_history: List[str]
    documents: List[Document]
    generation: str
    web_search_needed: str
    user_constraints: dict
    constraint_warning: str
    modification_history: list  # ë ˆì‹œí”¼ ìˆ˜ì • ì´ë ¥


def create_chat_agent(rag_system):
    """Chat Agent ìƒì„± - Adaptive RAG + ë„¤ì´ë²„ ê²€ìƒ‰"""

    search_engine = os.getenv("SEARCH_ENGINE", "serper")
    search_service = get_search_service(search_engine)
    print(f"[Agent] ê²€ìƒ‰ ì—”ì§„: {search_engine}")

    # ===== ë…¸ë“œ í•¨ìˆ˜ =====

    def rewrite_query(state: ChatAgentState) -> ChatAgentState:
        """ì¿¼ë¦¬ ì¬ì‘ì„±"""
        print("[Agent] ì¿¼ë¦¬ ì¬ì‘ì„± ì¤‘...")

        question = state["question"]
        history = state.get("chat_history", [])

        formatted_history = "\n".join(history[-5:]) if isinstance(history, list) else str(history)

        try:
            from langchain_naver import ChatClovaX
            llm = ChatClovaX(model="HCX-003", temperature=0.1, max_tokens=50)
            chain = REWRITE_PROMPT | llm
            better_question = chain.invoke({
                "history": formatted_history,
                "question": question
            })
            print_token_usage(better_question, "ì¿¼ë¦¬ ì¬ì‘ì„±")

            print(f"   ì›ë³¸: {question}")
            print(f"   ì¬ì‘ì„±: {better_question.content}")

            return {
                "question": better_question.content,
                "original_question": question
            }

        except Exception as e:
            print(f"   ì¬ì‘ì„± ì‹¤íŒ¨: {e}")
            return {
                "question": question,
                "original_question": question
            }

    def retrieve(state: ChatAgentState) -> ChatAgentState:
        """RAG ê²€ìƒ‰ (Reranker ì‚¬ìš©)"""
        print("[Agent] RAG ê²€ìƒ‰ ì¤‘...")

        question = state["question"]

        # use_rerank=None -> RAG ì‹œìŠ¤í…œ ì„¤ì •(USE_RERANKER) ë”°ë¦„
        results = rag_system.search_recipes(question, k=3, use_rerank=None)

        documents = [
            Document(
                page_content=doc.get("content", ""),
                metadata={
                    "title": doc.get("title", ""),
                    "cook_time": doc.get("cook_time", ""),
                    "level": doc.get("level", "")
                }
            )
            for doc in results
        ]

        print(f"   ê²€ìƒ‰ ê²°ê³¼: {len(documents)}ê°œ")
        for i, doc in enumerate(documents[:3], 1):
            print(f"   {i}. {doc.metadata.get('title', '')[:40]}...")

        return {"documents": documents}

    def check_constraints(state: ChatAgentState) -> ChatAgentState:
        """ì œì•½ ì¡°ê±´ ì²´í¬ (ì•Œë ˆë¥´ê¸°, ë¹„ì„ í˜¸ ìŒì‹)"""
        print("[Agent] ì œì•½ ì¡°ê±´ ì²´í¬ ì¤‘...")

        question = state["question"]
        user_constraints = state.get("user_constraints", {})

        if not user_constraints:
            print("   ì œì•½ ì¡°ê±´ ì—†ìŒ â†’ ìŠ¤í‚µ")
            return {"constraint_warning": ""}

        dislikes = user_constraints.get("dislikes", [])
        allergies = user_constraints.get("allergies", [])

        question_lower = question.lower()
        warning_parts = []

        for allergy in allergies:
            if allergy.lower() in question_lower:
                warning_parts.append(f"**{allergy}**ëŠ” ì•Œë ˆë¥´ê¸° ì¬ë£Œì…ë‹ˆë‹¤!")

        for dislike in dislikes:
            if dislike.lower() in question_lower:
                warning_parts.append(f"**{dislike}**ëŠ” ì‹«ì–´í•˜ëŠ” ìŒì‹ì…ë‹ˆë‹¤.")

        if warning_parts:
            warning_msg = "\n".join(warning_parts)
            print(f"   ì œì•½ ì¡°ê±´ ìœ„ë°˜ ê°ì§€!")
            print(f"   {warning_msg}")
            return {"constraint_warning": warning_msg}
        else:
            print("   ì œì•½ ì¡°ê±´ í†µê³¼")
            return {"constraint_warning": ""}

    def grade_documents(state: ChatAgentState) -> ChatAgentState:
        """ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€"""
        print("[Agent] ê´€ë ¨ì„± í‰ê°€ ì¤‘...")

        question = state["question"]
        documents = state["documents"]

        if not documents:
            print("   ë¬¸ì„œ ì—†ìŒ â†’ ì›¹ ê²€ìƒ‰")
            return {"web_search_needed": "yes"}

        try:
            question_lower = question.lower()

            found_exact_match = False
            for doc in documents[:3]:
                title = doc.metadata.get("title", "").lower()
                if question_lower in title or any(
                    word in title
                    for word in question_lower.split()
                    if len(word) > 1
                ):
                    found_exact_match = True
                    break

            if not found_exact_match:
                print("   ì œëª© ë§¤ì¹­ ì‹¤íŒ¨ â†’ ì›¹ ê²€ìƒ‰")
                return {"web_search_needed": "yes"}

            context_text = "\n".join([
                f"- {doc.page_content[:200]}"
                for doc in documents[:3]
            ])

            from langchain_naver import ChatClovaX
            llm = ChatClovaX(model="HCX-003", temperature=0.1, max_tokens=10)
            chain = GRADE_PROMPT | llm
            score = chain.invoke({
                "question": question,
                "context": context_text
            })

            print_token_usage(score, "ê´€ë ¨ì„± í‰ê°€")

            print(f"   í‰ê°€: {score.content}")

            if "yes" in score.content.lower():
                print("   DB ì¶©ë¶„ â†’ ìƒì„±")
                return {"web_search_needed": "no"}
            else:
                print("   DB ë¶€ì¡± â†’ ì›¹ ê²€ìƒ‰")
                return {"web_search_needed": "yes"}

        except Exception as e:
            print(f"   í‰ê°€ ì‹¤íŒ¨: {e}")
            return {"web_search_needed": "yes"}

    def web_search(state: ChatAgentState) -> ChatAgentState:
        """ì›¹ ê²€ìƒ‰"""
        print("[Agent] ì›¹ ê²€ìƒ‰ ì‹¤í–‰ ì¤‘...")

        question = state["question"]
        documents = search_service.search(query=question, max_results=3)

        for i, doc in enumerate(documents, 1):
            print(f"\n   [ê²€ìƒ‰ ê²°ê³¼ {i}]")
            print(f"   ì œëª©: {doc.metadata.get('title', '')}")
            print(f"   ë‚´ìš©: {doc.page_content[:200]}...")

        return {"documents": documents}

    def summarize_web_results(state: ChatAgentState) -> ChatAgentState:
        """ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½"""
        print("[Agent] ì›¹ ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ ì¤‘...")

        question = state["question"]
        documents = state["documents"]

        if not documents:
            print("   ìš”ì•½í•  ë¬¸ì„œ ì—†ìŒ")
            return {"documents": documents}

        try:
            summarized_docs = []

            for i, doc in enumerate(documents, 1):
                # ê° ë¬¸ì„œë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½
                summarize_prompt = f"""ì§ˆë¬¸: {question}

ë‚´ìš©:
{doc.page_content[:800]}

**ìš”ì•½ (3ë¬¸ì¥, ì¬ë£Œ/ì‹œê°„/ë‚œì´ë„ ìœ„ì£¼, ê´‘ê³  ì œê±°, ì •í™•í•œ ì–‘ ìœ ì§€):**"""

                from langchain_naver import ChatClovaX
                from langchain_core.messages import HumanMessage
                llm = ChatClovaX(model="HCX-003", temperature=0.2, max_tokens=300)
                result = llm.invoke([HumanMessage(content=summarize_prompt)])
                summary = result.content.strip()

                summarized_doc = Document(
                    page_content=summary,
                    metadata=doc.metadata
                )
                summarized_docs.append(summarized_doc)

                print(f"   {i}. ìš”ì•½ ì™„ë£Œ: {summary[:50]}...")

            return {"documents": summarized_docs}

        except Exception as e:
            print(f"   ìš”ì•½ ì‹¤íŒ¨: {e}, ì›ë³¸ ì‚¬ìš©")
            return {"documents": documents}

    def generate(state: ChatAgentState) -> ChatAgentState:
        """ë‹µë³€ ìƒì„±"""
        print("[Agent] ë‹µë³€ ìƒì„± ì¤‘...")

        question = state["original_question"]
        documents = state["documents"]
        history = state.get("chat_history", [])
        constraint_warning = state.get("constraint_warning", "")
        user_constraints = state.get("user_constraints", {})

        formatted_history = "\n".join(history[-10:]) if isinstance(history, list) else str(history)

        # ì›¹ ê²€ìƒ‰ ê²°ê³¼ëŠ” ì´ë¯¸ ìš”ì•½ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì „ì²´ ì‚¬ìš©, DB ê²€ìƒ‰ ê²°ê³¼ëŠ” 800ìë¡œ ì œí•œ
        context_text = "\n\n".join([
            doc.page_content if len(doc.page_content) < 1000 else doc.page_content[:800]
            for doc in documents
        ])

        if constraint_warning:
            try:
                alt_prompt = f"""{constraint_warning}

    ê·¸ë˜ë„ ë ˆì‹œí”¼ë¥¼ ì›í•˜ì‹œë‚˜ìš”?
    ì•„ë‹ˆë©´ ë¹„ìŠ·í•œ ë‹¤ë¥¸ ì¬ë£Œë¡œ ëŒ€ì²´í• ê¹Œìš”?

    ë‹µë³€:"""

                from langchain_core.messages import HumanMessage
                result = rag_system.chat_model.invoke([HumanMessage(content=alt_prompt)])
                answer = f"{constraint_warning}\n\n{result.content.strip()}"

                return {"generation": answer}

            except Exception as e:
                print(f"   ê²½ê³  ìƒì„± ì‹¤íŒ¨: {e}")
                return {"generation": f"{constraint_warning}\n\në‹¤ë¥¸ ìš”ë¦¬ë¥¼ ì¶”ì²œí•´ë“œë¦´ê¹Œìš”?"}

        try:
            # ì œì•½ ì¡°ê±´ì„ ì§ˆë¬¸ì— í†µí•© (ì»¨í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ì§ˆë¬¸ì— í¬í•¨)
            enhanced_question = question
            if user_constraints:
                allergies = user_constraints.get("allergies", [])
                dislikes = user_constraints.get("dislikes", [])

                constraints = []
                if allergies:
                    constraints.append(f"ì œì™¸: {', '.join(allergies)}")
                if dislikes:
                    constraints.append(f"ë¹„ì„ í˜¸: {', '.join(dislikes)}")

                if constraints:
                    enhanced_question = f"{question} ({' / '.join(constraints)})"

            # ì¸ì›ìˆ˜ ê³„ì‚° (ì„ íƒí•œ ê°€ì¡± êµ¬ì„±ì› ìˆ˜)
            servings = 1  # ê¸°ë³¸ê°’
            if user_constraints:
                names = user_constraints.get("names", [])
                if names and len(names) > 0:
                    servings = len(names)

            print(f"   [ì¸ì›ìˆ˜] {servings}ì¸ë¶„ìœ¼ë¡œ ë ˆì‹œí”¼ ìƒì„±")

            # ìˆ˜ì • ì´ë ¥ ì²˜ë¦¬ (ì¬ìƒì„± ì‹œ ì´ì „ ìˆ˜ì •ì‚¬í•­ ë°˜ì˜)
            # "ë¹¼ë‹¬ë¼"ê±°ë‚˜ "ì—†ëŠ”" ì¬ë£Œë§Œ ë°˜ì˜ (ì¶”ê°€ ìš”ì²­ì€ ì œì™¸)
            modification_history = state.get("modification_history", [])
            modification_constraints = ""

            print(f"\n{'='*60}")
            print(f"[ìˆ˜ì • ì´ë ¥ í™•ì¸] ì „ì²´ ìˆ˜ì • ì´ë ¥: {len(modification_history)}ê°œ")
            if modification_history:
                for i, mod in enumerate(modification_history, 1):
                    print(f"  [{i}] type={mod.get('type')}, request='{mod.get('request')}', remove={mod.get('remove_ingredients', [])}, add={mod.get('add_ingredients', [])}")

            if modification_history and len(modification_history) > 0:
                constrained_ingredients = []
                allowed_ingredients = []  # replace íƒ€ì…ì—ì„œ ì¶”ê°€ëœ ì¬ë£Œ (ì œì•½ í•´ì œ)
                filtered_out = []

                for mod in modification_history:
                    mod_type = mod.get("type")

                    # remove(ë¹¼ê¸°) ë˜ëŠ” replace(ëŒ€ì²´)ë§Œ ë°˜ì˜
                    if mod_type in ["remove", "replace"]:
                        remove_items = mod.get("remove_ingredients", [])
                        add_items = mod.get("add_ingredients", [])

                        if remove_items:
                            # ì œê±°í•  ì¬ë£Œë¥¼ ì œì•½ì‚¬í•­ì— ì¶”ê°€
                            constrained_ingredients.extend(remove_items)
                            print(f"  ì œì•½ ì¶”ê°€: {remove_items} (type={mod_type})")

                        if add_items and mod_type == "replace":
                            # replaceì˜ ê²½ìš° ì¶”ê°€ëœ ì¬ë£ŒëŠ” ì œì•½ í•´ì œ
                            allowed_ingredients.extend(add_items)
                            print(f"  ì œì•½ í•´ì œ: {add_items} (type={mod_type}, ì´ì œ ì‚¬ìš© ê°€ëŠ¥)")

                        if not remove_items and not add_items:
                            # ì¬ë£Œ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì œì•½ì‚¬í•­ì— ì¶”ê°€í•˜ì§€ ì•ŠìŒ
                            print(f"  ì¬ë£Œ ì¶”ì¶œ ì‹¤íŒ¨ë¡œ ì œì•½ì‚¬í•­ ì¶”ê°€ ìŠ¤í‚µ: '{mod['request']}' (type={mod_type})")
                    else:
                        filtered_out.append(mod['request'])
                        print(f"  ì œì™¸: {mod['request']} (type={mod_type})")

                # ì œì•½ í•´ì œëœ ì¬ë£ŒëŠ” ì œì•½ì‚¬í•­ì—ì„œ ì œê±°
                if allowed_ingredients:
                    print(f"\n[ì œì•½ í•´ì œ] {allowed_ingredients} â†’ ì œì•½ì‚¬í•­ì—ì„œ ì œê±°")
                    constrained_ingredients = [
                        ing for ing in constrained_ingredients
                        if ing not in allowed_ingredients
                    ]

                # ì¤‘ë³µ ì œê±°
                constrained_ingredients = list(set(constrained_ingredients))

                if constrained_ingredients:
                    # ì¬ë£Œëª… ë¦¬ìŠ¤íŠ¸ë¡œ ì œì•½ì‚¬í•­ ë¬¸êµ¬ ìƒì„±
                    ingredients_text = ", ".join(constrained_ingredients)
                    modification_constraints = f"\n**ì´ì „ ìˆ˜ì •ì‚¬í•­ (ë°˜ë“œì‹œ ë°˜ì˜):**\n- ì œì™¸: {ingredients_text}\n"
                    print(f"\n[ìµœì¢… ì œì•½ì‚¬í•­] {len(constrained_ingredients)}ê°œ ì¬ë£Œ ë°˜ì˜ë¨: {ingredients_text}")
                else:
                    print(f"\n[ìµœì¢… ì œì•½ì‚¬í•­] ë°˜ì˜í•  ì œì•½ì‚¬í•­ ì—†ìŒ (ëª¨ë‘ í•„í„°ë§ë¨)")

                if filtered_out:
                    print(f"[ì œì™¸ëœ ìˆ˜ì •ì‚¬í•­] {len(filtered_out)}ê°œ: {', '.join(filtered_out)}")
            else:
                print(f"[ìµœì¢… ì œì•½ì‚¬í•­] ìˆ˜ì • ì´ë ¥ ì—†ìŒ")
            print(f"{'='*60}\n")

            # max_tokens ëª…ì‹œì  ì„¤ì • (í† í° ì ˆì•½)
            from langchain_naver import ChatClovaX
            llm = ChatClovaX(model="HCX-003", temperature=0.3, max_tokens=500)
            chain = GENERATE_PROMPT | llm
            answer = chain.invoke({
                "context": context_text,
                "question": enhanced_question,
                "history": formatted_history,
                "servings": servings,
                "modification_constraints": modification_constraints  # ìˆ˜ì • ì œì•½ì‚¬í•­ ì¶”ê°€
            })

            print_token_usage(answer, "ë‹µë³€ ìƒì„±")

            # í›„ì²˜ë¦¬: ì¡°ë¦¬ë²• ì œê±° (ì±„íŒ…ìš©, ì¬ë£Œë§Œ ì¶œë ¥)
            # "ì¡°ë¦¬ë²•:" ë˜ëŠ” "1. " ë¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„ ì´í›„ ì œê±°
            import re

            # ì¡°ë¦¬ë²• ì„¹ì…˜ ì°¾ê¸° (ì—¬ëŸ¬ íŒ¨í„´ ì§€ì›)
            cooking_patterns = [
                r'\nì¡°ë¦¬ë²•[\s:ï¼š]+.*',  # "ì¡°ë¦¬ë²•:" ë˜ëŠ” "ì¡°ë¦¬ë²• :"
                r'\n\d+\.\s+.*',  # "1. " ë¡œ ì‹œì‘í•˜ëŠ” ì¤„ë“¤
                r'\n\*\*ì¡°ë¦¬ë²•\*\*[\s:ï¼š]+.*',  # "**ì¡°ë¦¬ë²•:**"
            ]

            cleaned_answer = answer.content
            for pattern in cooking_patterns:
                # í•´ë‹¹ íŒ¨í„´ë¶€í„° ëê¹Œì§€ ì œê±°
                match = re.search(pattern, cleaned_answer, re.DOTALL | re.IGNORECASE)
                if match:
                    cleaned_answer = cleaned_answer[:match.start()].strip()
                    print(f"   [í›„ì²˜ë¦¬] ì¡°ë¦¬ë²• ì œê±°ë¨")
                    break

            # ì•Œë ˆë¥´ê¸°/ë¹„ì„ í˜¸ ê´€ë ¨ í…ìŠ¤íŠ¸ ì œê±° (ì¶œë ¥ì— í¬í•¨ë˜ë©´ ì•ˆë¨)
            allergy_patterns = [
                r'\*ì•Œë ˆë¥´ê¸°.*?\n',  # "*ì•Œë ˆë¥´ê¸° ì¬ë£Œ ..."
                r'ì•Œë ˆë¥´ê¸° ì¬ë£Œ.*?\n',  # "ì•Œë ˆë¥´ê¸° ì¬ë£Œ (ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€): ..."
                r'ë¹„ì„ í˜¸ ìŒì‹.*?\n',  # "ë¹„ì„ í˜¸ ìŒì‹ (í”¼í•´ì•¼ í•¨): ..."
            ]

            for pattern in allergy_patterns:
                cleaned_answer = re.sub(pattern, '', cleaned_answer, flags=re.IGNORECASE)

            # ì†Œê°œ ë¬¸êµ¬ ì •ì œ: ì´ëª¨í‹°ì½˜, ìºì£¼ì–¼ í‘œí˜„ ì œê±°
            if '**ì†Œê°œ:**' in cleaned_answer:
                # ì†Œê°œ ì„¹ì…˜ ì¶”ì¶œ
                intro_match = re.search(r'\*\*ì†Œê°œ:\*\*\s*(.+?)(?:\n\*\*|$)', cleaned_answer, re.DOTALL)
                if intro_match:
                    intro_text = intro_match.group(1).strip()

                    # ì´ëª¨í‹°ì½˜ ì œê±° (á„’.á„’, á„á„, :), ^^, ë“±)
                    intro_text = re.sub(r'[á„€-á„’]{2,}', '', intro_text)  # á„á„, á„’á„’ ë“±
                    intro_text = re.sub(r'[:;]\)|:\(|:\)|^^|ã…ã…|ã…‹ã…‹', '', intro_text)

                    # ìºì£¼ì–¼ í‘œí˜„ ì œê±°
                    casual_phrases = [
                        r'ì•Œë ¤ë“œë¦´ê²Œìš”[!\s]*',
                        r'ë“œë¦´ê²Œìš”[!\s]*',
                        r'[~]+',
                        r'ìš”[~]+',
                        r'ë‹µë‹ˆë‹¤[:\s]*\)',
                        r'í•˜ì£ [!\s]*',
                        r'ê·¸ë§Œí¼.*?ìˆë‹µë‹ˆë‹¤',
                        r'ë ˆì‹œí”¼ë¥¼ ì•Œë ¤ë“œë¦´ê²Œìš”',
                        r'ì†Œê°œí•´ë“œë¦´ê²Œìš”',
                    ]
                    for phrase in casual_phrases:
                        intro_text = re.sub(phrase, '', intro_text)

                    # ë‹¤ì¤‘ ê³µë°± ì •ë¦¬
                    intro_text = re.sub(r'\s+', ' ', intro_text).strip()

                    # ë§ˆì¹¨í‘œë¡œ ëë‚˜ì§€ ì•Šìœ¼ë©´ ì¶”ê°€
                    if intro_text and not intro_text.endswith('.'):
                        intro_text += '.'

                    # ì†Œê°œ ë¬¸êµ¬ êµì²´
                    cleaned_answer = re.sub(
                        r'\*\*ì†Œê°œ:\*\*\s*.+?(?=\n\*\*|$)',
                        f'**ì†Œê°œ:** {intro_text}',
                        cleaned_answer,
                        count=1,
                        flags=re.DOTALL
                    )
                    print(f"   [í›„ì²˜ë¦¬] ì†Œê°œ ì •ì œë¨: {intro_text[:50]}...")

            # ì¬ë£Œ í˜•ì‹ ì •ë¦¬: ì¤„ë°”ê¿ˆ ì œê±°, ì‰¼í‘œë¡œ ë³€í™˜
            # "- ì¬ë£Œëª… ì–‘" í˜•ì‹ì„ "ì¬ë£Œëª… ì–‘," í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            if '**ì¬ë£Œ:**' in cleaned_answer:
                # ì¬ë£Œ ì„¹ì…˜ ì¶”ì¶œ
                parts = cleaned_answer.split('**ì¬ë£Œ:**')
                if len(parts) == 2:
                    before_ingredients = parts[0]
                    ingredients_section = parts[1].strip()

                    # ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„ëœ ì¬ë£Œë“¤ì„ ì‰¼í‘œë¡œ ë³€í™˜
                    # "- ì¬ë£Œëª… ì–‘" â†’ "ì¬ë£Œëª… ì–‘"
                    ingredients_lines = []
                    for line in ingredients_section.split('\n'):
                        line = line.strip()
                        if line and not line.startswith('**'):  # ë‹¤ìŒ ì„¹ì…˜ ì‹œì‘ ì „ê¹Œì§€
                            # "- " ì œê±°
                            line = re.sub(r'^[-\*]\s*', '', line)
                            if line:
                                # "ì•½ê°„", "ì ë‹¹ëŸ‰" ë“± ì• ë§¤í•œ í‘œí˜„ í¬í•¨ ì‹œ ì œì™¸
                                vague_terms = ['ì•½ê°„', 'ì ë‹¹ëŸ‰', 'ì¡°ê¸ˆ', 'ë„‰ë„‰íˆ', 'ì¶©ë¶„íˆ', 'ì ì ˆíˆ', 'ì·¨í–¥ê»', 'ì†ŒëŸ‰', 'ë‹¤ëŸ‰']
                                if any(term in line for term in vague_terms):
                                    print(f"   [í›„ì²˜ë¦¬] ì• ë§¤í•œ í‘œí˜„ í¬í•¨ ì¬ë£Œ ì œì™¸: {line}")
                                    continue

                                # ì–‘ì´ ì—†ëŠ” ì¬ë£Œ í•„í„°ë§ (ë°ì½”, í† í•‘ ë“±)
                                # ìˆ«ìë‚˜ ì–‘ ë‹¨ìœ„ê°€ ì—†ìœ¼ë©´ ì œì™¸
                                if not re.search(r'\d+|[ê°€-í£]+ìŠ¤í‘¼|ì‘ì€ìˆ |í°ìˆ |ì»µ|ê°œ|ëŒ€|ml|g|kg|L|ë°©ìš¸|ê¼¬ì§‘', line):
                                    print(f"   [í›„ì²˜ë¦¬] ì–‘ ì—†ëŠ” ì¬ë£Œ ì œì™¸: {line}")
                                    continue
                                ingredients_lines.append(line)
                        elif line.startswith('**'):
                            # ë‹¤ìŒ ì„¹ì…˜ ë°œê²¬, ì¤‘ë‹¨
                            break

                    # ì‰¼í‘œë¡œ ì—°ê²°
                    ingredients_text = ', '.join(ingredients_lines)

                    # ì¬êµ¬ì„±
                    cleaned_answer = f"{before_ingredients}**ì¬ë£Œ:** {ingredients_text}"
                    print(f"   [í›„ì²˜ë¦¬] ì¬ë£Œ í˜•ì‹ ì •ë¦¬ë¨")

            print(f"   ìƒì„± ì™„ë£Œ: {cleaned_answer[:50]}...")
            return {"generation": cleaned_answer}

        except Exception as e:
            print(f"   ìƒì„± ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return {"generation": "ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."}

    # ===== ê·¸ë˜í”„ êµ¬ì„± =====

    def decide_to_generate(state: ChatAgentState) -> Literal["web_search", "generate"]:
        """grade ë…¸ë“œ ì´í›„ ë¶„ê¸° ê²°ì •"""
        if state.get("web_search_needed") == "yes":
            return "web_search"
        else:
            return "generate"

    workflow = StateGraph(ChatAgentState)

    # â”€â”€ ëª¨ë“  ë…¸ë“œë¥¼ timed_nodeë¡œ ê°ì‹¸ê¸° â”€â”€
    workflow.add_node("rewrite",          timed_node("rewrite",          rewrite_query))
    workflow.add_node("retrieve",         timed_node("retrieve",         retrieve))
    workflow.add_node("check_constraints",timed_node("check_constraints",check_constraints))
    workflow.add_node("grade",            timed_node("grade",            grade_documents))
    workflow.add_node("web_search",       timed_node("web_search",       web_search))
    # workflow.add_node("summarize",        timed_node("summarize",        summarize_web_results))  # ì œê±°: ì‹œê°„ ì ˆì•½
    workflow.add_node("generate",         timed_node("generate",         generate))

    workflow.set_entry_point("rewrite")

    workflow.add_edge("rewrite", "retrieve")
    workflow.add_edge("retrieve", "check_constraints")
    workflow.add_edge("check_constraints", "grade")

    workflow.add_conditional_edges(
        "grade",
        decide_to_generate,
        {"web_search": "web_search", "generate": "generate"}
    )

    workflow.add_edge("web_search", "generate")  # ì§ì ‘ ì—°ê²° (ìš”ì•½ ìŠ¤í‚µ)
    # workflow.add_edge("web_search", "summarize")  # ì œê±°
    # workflow.add_edge("summarize", "generate")    # ì œê±°
    workflow.add_edge("generate", END)

    compiled = workflow.compile()

    print("[Agent] Adaptive RAG Agent ìƒì„± ì™„ë£Œ")
    print(f"[Agent] ê²€ìƒ‰ ì—”ì§„: {search_engine}")
    return compiled